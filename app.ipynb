{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload docs and query\n",
    "#upload audio file and query\n",
    "#provide url and query page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "llm = GooglePalm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ring-ding-ding-ding-ding-ding\n",
      "\n",
      "Wa-pa-pa-pa-pa-pa-pa\n",
      "\n",
      "Mow-mow-mow-mow-mow-mow\n",
      "\n",
      "Nana-na-na-na-na-na\n",
      "\n",
      "Eh-eh-eh-eh-eh-eh\n",
      "\n",
      "Ting-a-ling-a-ling-a-ling-a-ling\n",
      "a character in a poem\n"
     ]
    }
   ],
   "source": [
    "# test llm\n",
    "prompts = [\"what does the fox say?\",'who is humpty dumpty?'] # according to the class prmpts must be in list\n",
    "llm_result = llm._generate(prompts)\n",
    "\n",
    "print(llm_result.generations[0][0].text)\n",
    "print(llm_result.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import langchain dir loader from document loaders\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# directory path\n",
    "directory = 'data'\n",
    "# function to load the text docs\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use text splitter to split text in chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #This text splitter is the recommended one for generic text. It tries to split on them in order until the chunks are small enough\n",
    "\n",
    "# split the docs into chunks using recursive character splitter\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "# store the split documnets in docs variable\n",
    "docs = split_docs(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting With a Single PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our PDF into a document.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader('./data/RachelGreenCV.pdf')\n",
    "documents = pdf_loader.load() #This returns a list of Document’s, one Document for each page of the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Green\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm=llm)\n",
    "query = 'Who is the CV about?'\n",
    "response = chain.run(input_documents=documents, question=query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting With a Single PDF Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GooglePalmEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the document as before\n",
    "loader = PyPDFLoader('./data/RachelGreenCV.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split the data into chunks of 1,000 characters, with an overlap\n",
    "# of 200 characters between the chunks, which helps to give better results\n",
    "# and contain the context of the information between chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create our vectorDB, using the OpenAIEmbeddings tranformer to create\n",
    "# embeddings from our text chunks. We set all the db information to be stored\n",
    "# inside the ./data directory, so it doesn't clutter up our source files\n",
    "vectordb = Chroma.from_documents(\n",
    "  documents,\n",
    "  embedding=GooglePalmEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV is about a person named \"Green, R.\".\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 7}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# we can now execute queries against our Q&A chain\n",
    "result = qa_chain({'query': 'Who is the CV about?'})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "# qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "#     llm,\n",
    "#     vectordb.as_retriever(search_kwargs={'k': 6}),\n",
    "#     return_source_documents=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each question and answer, we will build up a list called chat_history , which we will pass back into the chain run command each time.\n",
    "# import sys\n",
    "\n",
    "# chat_history = []\n",
    "# while True:\n",
    "#     # this prints to the terminal, and waits to accept an input from the user\n",
    "#     query = input('Prompt: ')\n",
    "#     # give us a way to exit the script\n",
    "#     if query == \"exit\" or query == \"quit\" or query == \"q\":\n",
    "#         print('Exiting')\n",
    "#         sys.exit()\n",
    "#     # we pass in the query to the LLM, and print out the response. As well as\n",
    "#     # our query, the context of semantically relevant information from our\n",
    "#     # vector store will be passed in, as well as list of our chat history\n",
    "#     result = qa_chain({'question': query, 'chat_history': chat_history})\n",
    "#     print('Answer: ' + result['answer'])\n",
    "#     # we build up the chat_history list, based on our question and response\n",
    "#     # from the LLM, and the script then returns to the start of the loop\n",
    "#     # and is again ready to accept user input.\n",
    "#     chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting With Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir('data'):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path = './data/' + file\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.docx') or file.endswith('.doc'):\n",
    "        doc_path = './data/' + file\n",
    "        loader = Docx2txtLoader(doc_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.txt'):\n",
    "        text_path = './data/' + file\n",
    "        loader = TextLoader(text_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "chunked_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "  chunked_documents,\n",
    "  embedding=GooglePalmEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 7}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# we can now execute queries against our Q&A chain\n",
    "result = qa_chain({'query': 'how many cvs are there?'})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read All Docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import langchain dir loader from document loaders\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# directory path\n",
    "directory = 'data/'\n",
    "\n",
    "# function to load the text docs\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use text splitter to split text in chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #This text splitter is the recommended one for generic text. It tries to split on them in order until the chunks are small enough\n",
    "\n",
    "# split the docs into chunks using recursive character splitter\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "# store the split documnets in docs variable\n",
    "docs = split_docs(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TurnerZ\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# embeddings using langchain\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# using chromadb as a vector store and storing the docs in it\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using q&a chain to get the answer for our query\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Robert Roberts, Professor of English University of Illinois at Urbana-Champaign (217) 333-0203, rrobe3@illinois.edu\n",
      "\n",
      "Sally Briscoe, Assoc. Professor of English Butler University, Indianapolis, IN (317) 492-8763, briscoe@butler.edu\n",
      "\n",
      "Rachel Green, page 3 of 3\n",
      "\n",
      "5\n",
      "\n",
      "grad.illinois.edu/CareerDevelopment\n",
      "\n",
      "20xx-20xx\n",
      "\n",
      "Rachel Green, page 2 of 3\n",
      "\n",
      "4\n",
      "\n",
      "grad.illinois.edu/CareerDevelopment\n",
      "\n",
      "PROFESSIONAL SERVICE Managing Editor Southern Literary Journal   Oversee production and publication procedures.  Maintain editorial correspondence with prospective contributors. \n",
      "\n",
      "Process manuscripts submitted for publication\n",
      "\n",
      "Conduct business transactions including publicity, subscriptions and advertising.\n",
      "\n",
      "20xx-present\n",
      "\n",
      "Poetry Staff University Quarterly  Review and solicit poems for possible publication.\n",
      "\n",
      "20xx-present\n",
      "\n",
      "Editorial Assistant Southern Literary Journal  Designed and maintained journal’s internet presence. \n",
      "\n",
      "Edited copy for publication on a monthly basis.\n",
      "\n",
      "20xx-20xx\n",
      "\n",
      "UNIVERSITY SERVICE Graduate Mentor The Career Center, University of Illinois \n",
      "\n",
      "Counsel minority undergraduates on graduate programs, application procedures and funding.\n",
      "\n",
      "20xx-20xx\n",
      "\n",
      "Career Advisory Committee Graduate College, University of Illinois  \n",
      "\n",
      "with FEMA 356 Pre-standard for the Seismic Rehabilitation of buildings. 20XX TEACHING AND MENTORING EXPERIENCE\n",
      "\n",
      "Gonzalez, Gloria. “Yearning to Be Free: 3 Hispanic Women’s Diaries,” Hispanic Literature Today: 11(2): 18-31.\n",
      "\n",
      "CONFERENCE PRESENTATIONS\n",
      "\n",
      "2020. Gonzalez, Gloria. “Storytelling Methods in the Central Valley.” Hispanic Storytelling Association Annual Conference, San Francisco, CA\n",
      "\n",
      "2019. Gonzalez, Gloria. “When Cultures Merge: Themes of Exclusion in Mexican-American Literature.” US Hispanic Literature Annual Conference, Tucson, AZ.\n",
      "\n",
      "TEACHING EXPERIENCE\n",
      "\n",
      "\n",
      "\n",
      "Adjunct Lecturer, University of Houston\n",
      "\n",
      "Mexican-American Literature, Spanish 3331\n",
      "\n",
      "Women in Hispanic Literature, Spanish 3350\n",
      "\n",
      "Spanish-American Short Story, Spanish 4339 \n",
      "\n",
      "Graduate Teaching Assistant, Northwestern University \n",
      "\n",
      "Elementary Spanish 1501, 1502, 1505\n",
      "\n",
      "Intermediate Spanish 2301, 2302, 2610\n",
      "\n",
      "HONORS / AWARDS\n",
      "\n",
      "Mexico Study Abroad Summer Grant, 2018\n",
      "\n",
      "UH Teaching Awards, 2017, 2018, 2020\n",
      "\n",
      "Dissertation Fellowship, 2017 \n",
      "\n",
      "LANGUAGES\n",
      "\n",
      "English (native)\n",
      "\n",
      "Spanish (bilingual oral and written fluency)\n",
      "\n",
      "Classical Latin (written)\n",
      "\n",
      "Question: who has the highest job experience?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Robert Roberts'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your query and perform similarity search to generate an answer\n",
    "query = \"who has the highest job experience?\"\n",
    "matching_docs = db.similarity_search(query)\n",
    "answer =  chain.run(input_documents=matching_docs, question=query)\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
